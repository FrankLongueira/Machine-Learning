
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Mini_Matlab_3</title><meta name="generator" content="MATLAB 8.2"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2015-11-05"><meta name="DC.source" content="Mini_Matlab_3.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, tt, code { font-size:12px; }
pre { margin:0px 0px 20px; }
pre.error { color:red; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h2>Contents</h2><div><ul><li><a href="#2">Generative Model Classifier</a></li><li><a href="#3">Logistic Regression</a></li><li><a href="#4">Plotting</a></li></ul></div><pre class="codeinput"><span class="comment">% Machine Learning with Professor Keene</span>
<span class="comment">% Mini Matlab 3: Classification</span>
<span class="comment">% By Frank Longueira &amp; Matthew Smarsch</span>

clc;
clear <span class="string">all</span>;
close <span class="string">all</span>;
</pre><h2>Generative Model Classifier<a name="2"></a></h2><pre class="codeinput">load(<span class="string">'data.mat'</span>);   <span class="comment">% Load datasets</span>

x = {bimodal.x circles.x spiral.x unimodal.x};    <span class="comment">% Put fields into a cell array to access data easily</span>
targ = {bimodal.y circles.y spiral.y unimodal.y}; <span class="comment">% Cell array of dataset labels</span>

N = 400;        <span class="comment">% Number of total observations</span>
N1 = 200;       <span class="comment">% Total observations in Class 1</span>
N2 = 400 - N1;  <span class="comment">% Total observations in Class 2</span>
Pr_C1 = N1/N;   <span class="comment">% Probability of Class 1</span>
Pr_C2 = N2/N;   <span class="comment">% Probability of Class 2</span>

class1_mean = zeros(4,2);   <span class="comment">% Pre-allocate memory to store means given in Class 1 for each data set</span>
class2_mean = zeros(4,2);   <span class="comment">% Pre-allocate memory to store means given in Class 2 for each data set</span>

S1  = cell(4,1);   <span class="comment">% Pre-allocating memory to store covariance matrices and weight vectors</span>
S2  = cell(4,1);
S  = cell(4,1);
w = cell(4,1);
w0 = zeros(4,1);
phix1 = -8:0.1:8;  <span class="comment">% Vector used for ployting the decision hyperplane</span>
s = 0.5;           <span class="comment">% Gaussian basis function parameter</span>

<span class="comment">% Map Circle Data to Another 2 Dimensional Space to Make it Linearly Separable</span>
phi_circles = exp(-0.5*((repmat([1;1],1,N)-x{2}').^2)/s)'; <span class="comment">% Points of circles mapped to another space using Gaussian basis functions</span>

<span class="comment">% Map Spiral Data to Another 2 Dimensional Space to Make it Linearly Separable</span>
a_mid = (6.95555)/(4*pi);         <span class="comment">% Constructing a spiral that separates both spirals</span>
t_phi = linspace(0, 4*pi,400);    <span class="comment">% Domain of spiral parameter</span>
x1 = (a_mid.*t_phi.*cos(t_phi))'; <span class="comment">% x1 points of dividing spiral</span>
x2 = (-a_mid.*t_phi.*sin(t_phi))';<span class="comment">% x2 points of dividing spiral</span>
mag_mid = sqrt(x1.^2 + x2.^2);    <span class="comment">% Radius of spiral at (x1,x2)</span>
magdata = sqrt(x{3}(:,1).^2 + x{3}(:,2).^2); <span class="comment">% Radius of data points being mapped</span>
z = mag_mid - magdata;            <span class="comment">% Take difference between radius of dividing spiral and the data spirals.. This separates the data.</span>

phi_spiral = [x{3}(:,1) z];       <span class="comment">% Matrix of spiral points that have been mapped to a new two-dimensional space</span>

<span class="comment">% Map Bimodal Data to Another 2 Dimensional Space to Make it Linearly Separable</span>
blue_mean = mean(x{1}(1:200,:));  <span class="comment">% Mean of Class 1 Bimodal</span>
red_mean = mean(x{1}(201:400,:)); <span class="comment">% Mean of Class 2 Bimodal</span>
slope = (red_mean(2)-blue_mean(2))/(red_mean(1)-blue_mean(1)); <span class="comment">% Slope of line intersecting the data</span>
x1 = -7:0.01:7;                             <span class="comment">% Domain of line intersecting the data</span>
x2 = slope*(x1-red_mean(2)) + red_mean(2);  <span class="comment">% Range of line intersecting the data</span>

blue1 = [-7 -3];    <span class="comment">% Partition intervals based on spread of data</span>
red1 =  [-3 -1.155];
blue2 = [-1.155 1.039];
red2 = [1.039 5];

line = [x1' x2'];         <span class="comment">% (x1,x2) points of the line intersecting the data</span>
phi_bimodal = zeros(N,2); <span class="comment">% Allocate memory</span>
guess = zeros(N,1);       <span class="comment">% Allocate memory</span>
min_dist = @(xmin,ymin)   sum(((line-repmat([xmin ymin],length(x1),1)).^2)')'; <span class="comment">% Set up function for finding distance of a data point and every point on the line</span>

<span class="comment">% We now map each bimodal data point into the disjoint paritions above by looking</span>
<span class="comment">% at a given point's orthogonal projection onto the line and associating the</span>
<span class="comment">% data point with the value of x1 for the point on the line that intersects the</span>
<span class="comment">% orthogonal projection from the data point</span>

 <span class="keyword">for</span> j = 1:N
    xbin = x{1}(j,1);           <span class="comment">% Take in first coordinate of the first bimodal data point</span>
    ybin = x{1}(j,2);           <span class="comment">% Take in second coordinate of the first bimodal data point</span>
    z = min_dist(xbin,ybin);    <span class="comment">% Find minimum distance between point and the line</span>
    t = find(z == min(z));      <span class="comment">% Find the index in which this minimum distance occurs</span>
    min_spot = x1(t);           <span class="comment">% Find the coordinate x1 of the intersection point of (x1,x2) with the orthogonal projection</span>
    <span class="keyword">if</span> ((min_spot &lt; -3) &amp; (min_spot &gt; -7)) || ((min_spot &lt; 1.039) &amp; (min_spot &gt; -1.155)) <span class="comment">% Project this minimum coordinate into a partition listed above</span>
        phi_bimodal(j,:) = [0 1+(abs(min_spot)/10)];
        guess(j) = 0;
    <span class="keyword">else</span>
        phi_bimodal(j,:) = [abs(min_spot)/10 0];
        guess(j) = 1;
    <span class="keyword">end</span>
 <span class="keyword">end</span>

<span class="comment">% Unimodal is approximately linearly separable (i.e. no need to map the data)</span>
phi_unimodal = x{4};

<span class="comment">% Collect all mapped data into one cell array corresponding to each dataset</span>
phi = {phi_bimodal phi_circles phi_spiral phi_unimodal};


<span class="comment">% Apply Generative algorithm to find weights for each dataset classification</span>
<span class="keyword">for</span> i=1:4
    class1_mean(i,:) = mean(phi{i}(1:200,:));   <span class="comment">% Mean vector of class 1</span>
    class2_mean(i,:) = mean(phi{i}(201:400,:)); <span class="comment">% Mean vector of class 2</span>
    S1{i} =  cov(phi{i}(1:200,:),1);            <span class="comment">% Covariance matrix between x1 and x2 in class 1</span>
    S2{i} =  cov(phi{i}(201:400,:),1);          <span class="comment">% Covariance matrix between x1 and x2 in class 2</span>
    S{i}  = (N1/N)*S1{i} + (N2/N)*S2{i};        <span class="comment">% Covariance matrix between coordinates x1 and x2 of all data points in dataset</span>
    invS  = inv(S{i});                          <span class="comment">% Find inverse covariance matrix</span>
    w{i}  = invS*(class1_mean(i,:)' - class2_mean(i,:)');   <span class="comment">% Calculate weight vector</span>
    w0(i) = -(1/2)*class1_mean(i,:)*invS*class1_mean(i,:)'+ (1/2)*class2_mean(i,:)*invS*class2_mean(i,:)' + log(Pr_C1/Pr_C2); <span class="comment">% Calculate w0</span>
    Decision_Surface{i} =  -(w{i}(1)*phix1 + w0(i))/w{i}(2);        <span class="comment">% Store decision surface for each classification, to be plotted</span>
<span class="keyword">end</span>

Percent_Correct_Bimodal_Generative = (N-sum(xor(guess,targ{1})))/N <span class="comment">% Percentage of bimodal dataset correctly separated</span>
Percent_Correct_Circles_Generative = 1                             <span class="comment">% Percentage of circles dataset correctly separated (i.e. linearly separable)</span>
Percent_Correct_Spiral_Generative = 1                              <span class="comment">% Percentage of spiral dataset correctly separated (i.e. linearly separable)</span>
Percent_Correct_Unimodal_Generative = (N - sum(xor(((w{4}'*x{4}'+w0(4))&lt;0)',targ{4})))/N <span class="comment">% Percentage of unimodal dataset correctly separated</span>
</pre><pre class="codeoutput">
Percent_Correct_Bimodal_Generative =

    0.8550


Percent_Correct_Circles_Generative =

     1


Percent_Correct_Spiral_Generative =

     1


Percent_Correct_Unimodal_Generative =

    0.9650

</pre><h2>Logistic Regression<a name="3"></a></h2><pre class="codeinput"><span class="comment">% Perform logistic regression as given in our textbook</span>

LogSigmoid = @(a) 1./(1+exp(-a)); <span class="comment">% Create logsigmoid function for ease of use in code</span>

<span class="keyword">for</span> j = 1:4
    Io = [ones(N,1) phi{j}];                <span class="comment">% Design matrix</span>
    weights = [0.1 0.1 0.1]';               <span class="comment">% Initialize weights</span>
    min_error = 1;                          <span class="comment">% Inititialize error</span>
    <span class="keyword">while</span> min_error &gt; 1e-3                  <span class="comment">% Continue optimizing using Newton Raphson until sufficient error received</span>
        error_old = min_error;              <span class="comment">% Store old error to avoid while loop not terminating</span>
        yn = LogSigmoid(weights'*Io')';     <span class="comment">% Applied the logsigmoid map to the linear sum input argument</span>
        error = yn - targ{j};               <span class="comment">% Error between target values and values mapped by the sigmoid</span>
        R = diag(yn.*(1-yn));               <span class="comment">% Diagonal matrix involving mapped data values</span>
        z = Io*weights - inv(R)*(error);    <span class="comment">% z vector used in updating weights</span>
        weights = inv(Io'*R*Io)*Io'*R*z;    <span class="comment">% Update weight vector</span>
        min_error = sum(error.^2)/N;        <span class="comment">% Retrieve overrall average error using the previous weights</span>
        <span class="keyword">if</span> min_error == error_old;          <span class="comment">% Avoid non-terminating while loop</span>
            <span class="keyword">break</span>
        <span class="keyword">elseif</span> det(R) &lt; 0.05                <span class="comment">% Avoid singular matrix occurs and breaking the algorithm</span>
            <span class="keyword">break</span>
        <span class="keyword">end</span>
    <span class="keyword">end</span>
    w_final{j} = weights;                   <span class="comment">% Final optimized weights at the end of the algorithm for each dataset</span>
    Log_Decision_Surface{j} =  -(w_final{j}(2)*phix1+w_final{j}(1))/(w_final{j}(3));    <span class="comment">% Decision surface using the just found weights from log regression</span>
<span class="keyword">end</span>

Percent_Correct_Bimodal_Regression = (N-sum(xor((w_final{1}(2:3)'*phi{1}'+w_final{1}(1)&gt;0)',targ{1})))/N    <span class="comment">% Percentage of bimodal dataset correctly separated</span>
Percent_Correct_Circles_Regression = 1                             <span class="comment">% Percentage of circles dataset correctly separated (i.e. linearly separable)</span>
Percent_Correct_Spiral_Regression = 1                              <span class="comment">% Percentage of spiral dataset correctly separated (i.e. linearly separable)</span>
Percent_Correct_Unimodal_Regression = (N - sum(xor(((w_final{4}(2:3)'*x{4}'+w_final{4}(1))&gt;0)',targ{4})))/N <span class="comment">% Percentage of unimodal dataset correctly separated</span>
</pre><pre class="codeoutput">
Percent_Correct_Bimodal_Regression =

    0.8550


Percent_Correct_Circles_Regression =

     1


Percent_Correct_Spiral_Regression =

     1


Percent_Correct_Unimodal_Regression =

    0.9700

</pre><h2>Plotting<a name="4"></a></h2><pre class="codeinput"><span class="comment">% Plot original data</span>
Titles = {<span class="string">'Bimodal Dataset'</span> <span class="string">'Circles Dataset'</span> <span class="string">'Spiral Dataset'</span> <span class="string">'Unimodal Dataset'</span>};
XLIMS = {[-0.1 0.5] [0 1.1] [-4 4] [-5 5]};
YLIMS = {[-0.2 1.8] [0.2 1.2] [-4 4] [-5 6]};

<span class="keyword">for</span> j = 1:4
    subplot(2,2,j);
    scatter(x{j}(1:200,1),x{j}(1:200,2),<span class="string">'b'</span>);
    hold <span class="string">on</span>
    scatter(x{j}(201:400,1),x{j}(201:400,2),<span class="string">'r'</span>);
    title(Titles{j});
    xlabel(<span class="string">'x1'</span>);
    ylabel(<span class="string">'x2'</span>);
    legend(<span class="string">'Class 1'</span>,<span class="string">'Class 2'</span>);
<span class="keyword">end</span>

<span class="comment">% Plot Generative Classifier Results</span>
Titles = {<span class="string">'Bimodal Generative Classifier'</span> <span class="string">'Circles Generative Classifier'</span> <span class="string">'Spiral Generative Classifier'</span> <span class="string">'Unimodal Generative Classifier'</span>};
figure
<span class="keyword">for</span> j = 1:4
    subplot(2,2,j);
    scatter(phi{j}(1:200,1),phi{j}(1:200,2),<span class="string">'b'</span>);
    hold <span class="string">on</span>
    scatter(phi{j}(201:400,1),phi{j}(201:400,2),<span class="string">'r'</span>);
    hold <span class="string">on</span>
    plot(phix1,Decision_Surface{j},<span class="string">'g'</span>);
    title(Titles{j});
    xlim(XLIMS{j})
    ylim(YLIMS{j})
    xlabel(<span class="string">'phi1'</span>);
    ylabel(<span class="string">'phi2'</span>);
    legend(<span class="string">'Class 1'</span>,<span class="string">'Class 2'</span>, <span class="string">'Decision Surface'</span>);
<span class="keyword">end</span>

<span class="comment">% Plot Log Regression Classifier Results</span>
Titles = {<span class="string">'Bimodal Log Regression Classifier'</span> <span class="string">'Circles Log Regression Classifier'</span> <span class="string">'Spiral Log Regression Classifier'</span> <span class="string">'Unimodal Log Regression Classifier'</span>};
figure
<span class="keyword">for</span> j = 1:4
    subplot(2,2,j);
    scatter(phi{j}(1:200,1),phi{j}(1:200,2),<span class="string">'b'</span>);
    hold <span class="string">on</span>
    scatter(phi{j}(201:400,1),phi{j}(201:400,2),<span class="string">'r'</span>);
    hold <span class="string">on</span>
    plot(phix1,Log_Decision_Surface{j},<span class="string">'g'</span>);
    title(Titles{j});
    xlabel(<span class="string">'phi1'</span>);
    ylabel(<span class="string">'phi2'</span>);
    legend(<span class="string">'Class 1'</span>,<span class="string">'Class 2'</span>, <span class="string">'Decision Surface'</span>);
    xlim(XLIMS{j})
    ylim(YLIMS{j})
<span class="keyword">end</span>
</pre><img vspace="5" hspace="5" src="Mini_Matlab_3_01.png" alt=""> <img vspace="5" hspace="5" src="Mini_Matlab_3_02.png" alt=""> <img vspace="5" hspace="5" src="Mini_Matlab_3_03.png" alt=""> <p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2013b</a><br></p></div><!--
##### SOURCE BEGIN #####
% Machine Learning with Professor Keene
% Mini Matlab 3: Classification
% By Frank Longueira & Matthew Smarsch

clc;
clear all;
close all;

%% Generative Model Classifier

load('data.mat');   % Load datasets

x = {bimodal.x circles.x spiral.x unimodal.x};    % Put fields into a cell array to access data easily
targ = {bimodal.y circles.y spiral.y unimodal.y}; % Cell array of dataset labels

N = 400;        % Number of total observations
N1 = 200;       % Total observations in Class 1
N2 = 400 - N1;  % Total observations in Class 2
Pr_C1 = N1/N;   % Probability of Class 1
Pr_C2 = N2/N;   % Probability of Class 2

class1_mean = zeros(4,2);   % Pre-allocate memory to store means given in Class 1 for each data set
class2_mean = zeros(4,2);   % Pre-allocate memory to store means given in Class 2 for each data set

S1  = cell(4,1);   % Pre-allocating memory to store covariance matrices and weight vectors
S2  = cell(4,1);
S  = cell(4,1);
w = cell(4,1);
w0 = zeros(4,1);
phix1 = -8:0.1:8;  % Vector used for ployting the decision hyperplane
s = 0.5;           % Gaussian basis function parameter

% Map Circle Data to Another 2 Dimensional Space to Make it Linearly Separable
phi_circles = exp(-0.5*((repmat([1;1],1,N)-x{2}').^2)/s)'; % Points of circles mapped to another space using Gaussian basis functions

% Map Spiral Data to Another 2 Dimensional Space to Make it Linearly Separable
a_mid = (6.95555)/(4*pi);         % Constructing a spiral that separates both spirals
t_phi = linspace(0, 4*pi,400);    % Domain of spiral parameter
x1 = (a_mid.*t_phi.*cos(t_phi))'; % x1 points of dividing spiral
x2 = (-a_mid.*t_phi.*sin(t_phi))';% x2 points of dividing spiral
mag_mid = sqrt(x1.^2 + x2.^2);    % Radius of spiral at (x1,x2)
magdata = sqrt(x{3}(:,1).^2 + x{3}(:,2).^2); % Radius of data points being mapped
z = mag_mid - magdata;            % Take difference between radius of dividing spiral and the data spirals.. This separates the data.

phi_spiral = [x{3}(:,1) z];       % Matrix of spiral points that have been mapped to a new two-dimensional space

% Map Bimodal Data to Another 2 Dimensional Space to Make it Linearly Separable
blue_mean = mean(x{1}(1:200,:));  % Mean of Class 1 Bimodal
red_mean = mean(x{1}(201:400,:)); % Mean of Class 2 Bimodal
slope = (red_mean(2)-blue_mean(2))/(red_mean(1)-blue_mean(1)); % Slope of line intersecting the data
x1 = -7:0.01:7;                             % Domain of line intersecting the data
x2 = slope*(x1-red_mean(2)) + red_mean(2);  % Range of line intersecting the data

blue1 = [-7 -3];    % Partition intervals based on spread of data
red1 =  [-3 -1.155];
blue2 = [-1.155 1.039];
red2 = [1.039 5];

line = [x1' x2'];         % (x1,x2) points of the line intersecting the data
phi_bimodal = zeros(N,2); % Allocate memory
guess = zeros(N,1);       % Allocate memory  
min_dist = @(xmin,ymin)   sum(((line-repmat([xmin ymin],length(x1),1)).^2)')'; % Set up function for finding distance of a data point and every point on the line

% We now map each bimodal data point into the disjoint paritions above by looking
% at a given point's orthogonal projection onto the line and associating the
% data point with the value of x1 for the point on the line that intersects the
% orthogonal projection from the data point

 for j = 1:N
    xbin = x{1}(j,1);           % Take in first coordinate of the first bimodal data point
    ybin = x{1}(j,2);           % Take in second coordinate of the first bimodal data point
    z = min_dist(xbin,ybin);    % Find minimum distance between point and the line
    t = find(z == min(z));      % Find the index in which this minimum distance occurs
    min_spot = x1(t);           % Find the coordinate x1 of the intersection point of (x1,x2) with the orthogonal projection
    if ((min_spot < -3) & (min_spot > -7)) || ((min_spot < 1.039) & (min_spot > -1.155)) % Project this minimum coordinate into a partition listed above
        phi_bimodal(j,:) = [0 1+(abs(min_spot)/10)];
        guess(j) = 0;
    else
        phi_bimodal(j,:) = [abs(min_spot)/10 0];
        guess(j) = 1;
    end
 end
    
% Unimodal is approximately linearly separable (i.e. no need to map the data)
phi_unimodal = x{4};

% Collect all mapped data into one cell array corresponding to each dataset
phi = {phi_bimodal phi_circles phi_spiral phi_unimodal};


% Apply Generative algorithm to find weights for each dataset classification
for i=1:4
    class1_mean(i,:) = mean(phi{i}(1:200,:));   % Mean vector of class 1
    class2_mean(i,:) = mean(phi{i}(201:400,:)); % Mean vector of class 2
    S1{i} =  cov(phi{i}(1:200,:),1);            % Covariance matrix between x1 and x2 in class 1
    S2{i} =  cov(phi{i}(201:400,:),1);          % Covariance matrix between x1 and x2 in class 2
    S{i}  = (N1/N)*S1{i} + (N2/N)*S2{i};        % Covariance matrix between coordinates x1 and x2 of all data points in dataset
    invS  = inv(S{i});                          % Find inverse covariance matrix
    w{i}  = invS*(class1_mean(i,:)' - class2_mean(i,:)');   % Calculate weight vector
    w0(i) = -(1/2)*class1_mean(i,:)*invS*class1_mean(i,:)'+ (1/2)*class2_mean(i,:)*invS*class2_mean(i,:)' + log(Pr_C1/Pr_C2); % Calculate w0
    Decision_Surface{i} =  -(w{i}(1)*phix1 + w0(i))/w{i}(2);        % Store decision surface for each classification, to be plotted
end

Percent_Correct_Bimodal_Generative = (N-sum(xor(guess,targ{1})))/N % Percentage of bimodal dataset correctly separated
Percent_Correct_Circles_Generative = 1                             % Percentage of circles dataset correctly separated (i.e. linearly separable)
Percent_Correct_Spiral_Generative = 1                              % Percentage of spiral dataset correctly separated (i.e. linearly separable)
Percent_Correct_Unimodal_Generative = (N - sum(xor(((w{4}'*x{4}'+w0(4))<0)',targ{4})))/N % Percentage of unimodal dataset correctly separated


%% Logistic Regression

% Perform logistic regression as given in our textbook

LogSigmoid = @(a) 1./(1+exp(-a)); % Create logsigmoid function for ease of use in code

for j = 1:4
    Io = [ones(N,1) phi{j}];                % Design matrix
    weights = [0.1 0.1 0.1]';               % Initialize weights
    min_error = 1;                          % Inititialize error
    while min_error > 1e-3                  % Continue optimizing using Newton Raphson until sufficient error received
        error_old = min_error;              % Store old error to avoid while loop not terminating
        yn = LogSigmoid(weights'*Io')';     % Applied the logsigmoid map to the linear sum input argument
        error = yn - targ{j};               % Error between target values and values mapped by the sigmoid
        R = diag(yn.*(1-yn));               % Diagonal matrix involving mapped data values
        z = Io*weights - inv(R)*(error);    % z vector used in updating weights
        weights = inv(Io'*R*Io)*Io'*R*z;    % Update weight vector
        min_error = sum(error.^2)/N;        % Retrieve overrall average error using the previous weights
        if min_error == error_old;          % Avoid non-terminating while loop
            break
        elseif det(R) < 0.05                % Avoid singular matrix occurs and breaking the algorithm
            break
        end
    end
    w_final{j} = weights;                   % Final optimized weights at the end of the algorithm for each dataset
    Log_Decision_Surface{j} =  -(w_final{j}(2)*phix1+w_final{j}(1))/(w_final{j}(3));    % Decision surface using the just found weights from log regression
end

Percent_Correct_Bimodal_Regression = (N-sum(xor((w_final{1}(2:3)'*phi{1}'+w_final{1}(1)>0)',targ{1})))/N    % Percentage of bimodal dataset correctly separated
Percent_Correct_Circles_Regression = 1                             % Percentage of circles dataset correctly separated (i.e. linearly separable)
Percent_Correct_Spiral_Regression = 1                              % Percentage of spiral dataset correctly separated (i.e. linearly separable)
Percent_Correct_Unimodal_Regression = (N - sum(xor(((w_final{4}(2:3)'*x{4}'+w_final{4}(1))>0)',targ{4})))/N % Percentage of unimodal dataset correctly separated
   
%% Plotting

% Plot original data
Titles = {'Bimodal Dataset' 'Circles Dataset' 'Spiral Dataset' 'Unimodal Dataset'};
XLIMS = {[-0.1 0.5] [0 1.1] [-4 4] [-5 5]};
YLIMS = {[-0.2 1.8] [0.2 1.2] [-4 4] [-5 6]};

for j = 1:4
    subplot(2,2,j);
    scatter(x{j}(1:200,1),x{j}(1:200,2),'b');
    hold on 
    scatter(x{j}(201:400,1),x{j}(201:400,2),'r');
    title(Titles{j});
    xlabel('x1');
    ylabel('x2');
    legend('Class 1','Class 2');
end

% Plot Generative Classifier Results
Titles = {'Bimodal Generative Classifier' 'Circles Generative Classifier' 'Spiral Generative Classifier' 'Unimodal Generative Classifier'};
figure
for j = 1:4
    subplot(2,2,j);
    scatter(phi{j}(1:200,1),phi{j}(1:200,2),'b');
    hold on 
    scatter(phi{j}(201:400,1),phi{j}(201:400,2),'r');
    hold on
    plot(phix1,Decision_Surface{j},'g');
    title(Titles{j});
    xlim(XLIMS{j})
    ylim(YLIMS{j})
    xlabel('phi1');
    ylabel('phi2');
    legend('Class 1','Class 2', 'Decision Surface');
end

% Plot Log Regression Classifier Results
Titles = {'Bimodal Log Regression Classifier' 'Circles Log Regression Classifier' 'Spiral Log Regression Classifier' 'Unimodal Log Regression Classifier'};
figure
for j = 1:4
    subplot(2,2,j);
    scatter(phi{j}(1:200,1),phi{j}(1:200,2),'b');
    hold on 
    scatter(phi{j}(201:400,1),phi{j}(201:400,2),'r');
    hold on
    plot(phix1,Log_Decision_Surface{j},'g');
    title(Titles{j});
    xlabel('phi1');
    ylabel('phi2');
    legend('Class 1','Class 2', 'Decision Surface');
    xlim(XLIMS{j})
    ylim(YLIMS{j})
end

##### SOURCE END #####
--></body></html>